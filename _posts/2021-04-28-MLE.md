We have a Bernaoulii distribution:
$$p(x\|\theta) = \begin{cases} \theta &\text{ if x = 1}\\ 1- \theta &\text{ if x = 0} \end{cases}$$

Expectation:
$$\mathbb{E}\[X\] = \sum xp(x\|\theta) = \theta$$

Varience:
$$\mathbb{E}\[X-\mathbb{E}\[X\]\]^2 = \mathbb{E}\[X-\theta\]^2 = (1-\theta)^2p(X=1\|\theta) + (0-\theta)^2p(X=0\|\theta) = (1-\theta)^2\theta + \theta^2(1-\theta) = \theta(1-\theta)$$

![]({{site.url}}/{{site.baseurl}}/assets/mle/var.png)

We see variance is high when uncertainty is maximum.

Variance is not only the measure of uncertainty, there is entrophy too.

Minimize uncertainty = maximize information

## Frequentist learning
- **assumes there is a true model with parameter $\theta_0$**
- probability is the frequency
- The estimate parameter is $\hat{\theta}$
- Given $n$ data points, $x_{1:n}= \{x_1, x_2, \dots \}$, we have to choose the $\theta$ that makes it more probable to make these data. $$ \hat{\theta} = \arg\max_{\theta} p(x_{1:n}\|\theta) $$

#### Procedures
- Given $n$ data points $x_{1:n} = \{x_1, x_2,\dots \}$, joint distribution of the data: $$p(x_{1:n}\|\theta) = \prod_{i=1}^n p(x_i\|\theta)$$
- log liklihood: $\mathcal{L}(\theta) = logP(x_{1:n}\|\theta) = \sum_{i=1}^n log(x_i\|theta)$
- differentiate and equal to zero to find estimate of $\theta$

## Bayesian Learning
- Given $n$ data points $x_{1:n} = \{x_1, x_2,\dots, x_n \}$
- Calculate the formula $p(x_{1:n}\|\theta) = \prod_{i=1}^n p(x_i\|\theta) = \theta^m(1-\theta)^{n-m}$
- Specify the prior: $p(\theta)$
- Compute the posterior: $$p(\theta\|x_{1:n}) = \frac{p(\theta)p(x_{1:n}\|\theta)}{p(x_{1:n})}$$
$$p(\theta\|x_{1:n}) \propto p(\theta)p(x_{1:n}\|\theta)$$
$$p(\theta\|x_{1:n}) = \frac{p(\theta)p(x_{1:n}\|\theta)}{\int p(x_{1:n}\|\theta)d\theta}$$

## Prior $\theta$ with beta distribution