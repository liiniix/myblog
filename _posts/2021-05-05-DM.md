## Tree Pruning
So that tree doesn't overfit to noisy data.
- Prepruning: Tree will be pruning as it is being generated. Tree will be partioned further if a threshold is satisfied during runtime.
- Postpruning: Pruning after tree is fully constructed. There will be a pruning set. We will evaluate accuracy by enabling and disabling a specific branch and keep a branch if doing well enabling the branch, pruning the branch otherwise.
- Pessimistic pruning: huge error rate, penalty is added.
- Repitation and Replication: Can be solved by multivariate splits, based on combination of attributes
![]({{site.url}}/{{site.baseurl}}/assets/dm/repet_rep.png)

## Scalibility and Decision Tree Induction
**What if disk data don't fit in memory?**

**Rainforest** Algorithm, uses AVC set instead of whole dataset:
![]({{site.url}}/{{site.baseurl}}/assets/dm/AVC.png)


**BOAT(Bootstrap optimistic algorithm for tree construction)**

- Create several samples of the training data which fit in the memory
- Each subset is used to constructed a tree
- The trees are combined resulting a new tree

## Visual Mining for Decision Tree Induction

**Perception-based classification (PBC)** is an interactive approach based on multidimensional visualization techniques and allows the user to incorporate background knowledge about the data when building a decision tree.

## Bayes Theorem
$$ X = \text{ Evidence} $$
$$ H = \text{ Hypothesis} $$
$$ P(H) = \text{ Prior} $$
$$ P(H\|X) = \text{ Posterior} $$
$$ P(H\|X) = \frac{P(H)P(X\|H)}{P(X)} $$