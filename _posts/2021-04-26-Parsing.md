`Ambiguity` is the problem.
- POS ambiguity
- Prepositional phrase attachment
- Noun premodifier

# Probabilistic Contex-Free Grammers
Here we have set of rules along with their associated probabilities:

$$ \text{Vt } \to \text{ Vt  NP   .4} $$

Probability of a tree with rules:
$$\alpha_1 \to \beta_1, \dots , \alpha_n \to \beta_n$$

$$p(t) = \prod_{i=1}^n q(\alpha_i \to \beta_i)$$

# Properties of PCFG

- We have a sentence $s$ which have different parse tree, set of all parse trees: $\mathcal{T}(s)$
- Most likely parse tree $$\arg\max_{t\in\mathcal{T}(s)} p(t) $$

If we enumerate all the possible trees to come up with tree with max propability, algorithm will grow exponentially.

For `CYK`, we have to make CFG into Chomsky normal form.

$$\pi\[X, i, j\] = \text{  maximum parsing tree for word i to j for non-terminal X}$$
![]({{site.url}}/{{site.baseurl}}/assets/pcfg/pcfg.jpg)

$$ \text{Base Case:  } \pi\[i, i, X] = q(X \to w_i) $$

$$ \pi\[i,j,X] = max_{X\to YZ \in R, s\in \\{i,...,(j-1)\\}}q(X\to YZ)\pi\[i, s, Y\]\pi[s+1, j, Z] $$

## Weakness of PCFG
- lack of sensitivity to lexical information, in pcfg we assumed that POS only is the greatest motivator
- lack of sensitivity to structrual frequencies

# Lexicalized PCFG
- One of the children is the dead of the rule
$$ X(h) \to_1 Y_1(h) Y_2(w) $$
$$ X(h) \to_2 Y_1(w) Y_2(h) $$

Video 42, 43, 45 dekhi nai(parameter estimation lexipcfg)